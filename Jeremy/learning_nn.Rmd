```{r}
library(keras)
# install_keras() # Uncomment this line and run it once to install the newest version of keras on your computer, then comment out this line again
library(dplyr)
```

# Read in the data

- Step 1: Read in the data and separate it out into training data and test data. The test data that we have reserved will only be used at the very end to evaluate the model.

- Step 2: Use a scaling method to standardize the numeric variables. Different numeric variables measure different things, hence they have different scalings. In order to maximize the neural network model, we have to standardize the numerical variables to the same scale. There are different standardization methods we can use, i.e. standard scaling, min-max scaling. We'll be using min-max scaling for this dataset.

- Step 3: Split the dataset into training and testing set. 

```{r}
# Step 1:
all_data <- read.csv('https://raw.githubusercontent.com/JeremyHub/STAT-456-Final/main/Jeremy/train.csv')

# Step 2: 
train <- all_data[,sapply(all_data, is.numeric)] %>%
  sapply(function(x) (x - min(x)) / (max(x) - min(x))) %>% # applies the function to perform min-max scaling
  as.data.frame() %>%
  select(-LotFrontage,-MasVnrArea,-GarageYrBlt) %>% # filters out all non-numeric columns as the model doesn't know how to handle strings
  na.omit()

test <- train[0:500,] # the first 500 rows belongs to the test set
train <- train[500:nrow(train),] # the rest of the dataset belongs to the training set
```

# Seperate out the output columns

For the training and testing data, we separate out the columns that will be used for output into and X and Y datasets. The X dataset will have all of the columns that are used as predictors, the Y dataset will be used to tell the model what the outcome was from the corresponding row in the X dataset.

```{r}
x_train <- train %>%
  select(-SalePrice,-Id) # keeps all columns except SalePrice and Id since they're not predictors
y_train <- train %>%
  select(SalePrice) # assigns SalePrice to y as an outcome variable
x_test <- test %>%
  select(-SalePrice,-Id)
y_test <- test %>%
  select(SalePrice)
```


# Reshaping the data

We then have to reshape the data into a 2D array and get rid of the column names. The 2D array's new shape must match the shape of the input layer of the neural network, which is 33 for this dataset. Reshaping the data is a crucial step in preparing the data for neural network training. It ensures that the input data matches the shape expected by the neural network and doesn't have any extra information like column names that the network doesn't know how to parse.


```{r}
# reshape
x_train <- x_train %>%
  unlist() %>%
  array(dim = c(nrow(x_train),33))

x_test <- x_test %>%
  unlist() %>%
  array(dim = c(nrow(x_test),33))
```

```{r}
y_train <- y_train %>%
  unlist() %>%
  array(dim=c(nrow(y_train),1))
y_test <- y_test %>%
  unlist() %>%
  array(dim=c(nrow(y_test),1))
```

# Build a neural network on the data!

- Step 1: Prepare the linear stack of layers using `keras_model_sequential()`

- Step 2: Use `layer_dense()` to add a densely-connected neural network layer to the framework built previously. 

+ `units` allows us to specify the number of neurons you want to use in this model. `units = 256` creates a dense layer with 256 neurons. 

+ `activation` allows us to determine whether a neuron should be activated or not (and how much weight to give that activation). Some common activation functions are ReLU, sigmoid, and tanh. 

+ `input_shape` is the dimensionality of the input aka the number of columns in the dataset. This argument is required when using this layer as the first layer in a model.

- Step 3: Use `layer_dropout()` to apply dropout to the input. Dropout is a technique used in neural networks to prevent overfitting. This technique drops out some neurons in the training set, which allows other neurons to be more well-rounded aka to pick up patterns to generalize on data they have never seen before. `rate` parameter ranges from 0 to 1, allowing us to specify the fraction of the neurons that we should drop along the way.


```{r}
model <- keras_model_sequential() 
model %>% 
  layer_dense(units = 256, activation = 'relu', input_shape = c(33)) %>% 
  layer_dropout(rate = 0.4) %>% 
  layer_dense(units = 128, activation = 'relu') %>%
  layer_dropout(rate = 0.3) %>%
  layer_dense(units = 64, activation = 'relu') %>%
  layer_dropout(rate = 0.2) %>%
  layer_dense(units = 1, activation = 'linear')
```

# Check the shape of our model

```{r}
summary(model)
```

# Compile the model

```{r}
model %>% compile(
  loss = 'mean_squared_error',
  optimizer = 'adam',
  metrics = c('mae')
)
```

# Fit the Model & Define Training Length

```{r}
history <- model %>% fit(
  x_train, y_train, 
  epochs = 300, batch_size = 256, 
  validation_split = 0.2
)
```

# Plot the Model History

```{r}
plot(history)
```

# Look at Results

```{r}
results <- model %>% evaluate(x_test, y_test)
results[2] * (max(all_data$SalePrice) - min(all_data$SalePrice)) + min(all_data$SalePrice)
```


## Exercise

Now try predicting a categorical variable from the following dataset. (make, model, etc.)

```{r}
cars_data <- read.csv("https://raw.githubusercontent.com/JeremyHub/STAT-456-Final/main/Jeremy/car%20details%20v4.csv")


cars_data_clean <- cars_data %>%
  separate(Max.Power, c("bhp_power", "rpm_power"), " @ ") %>%
  separate(Max.Torque, c("nm_torque", "rpm_torque"), " @ ") %>%
  mutate(across(11:15, gsub("n", "")))
head(cars_data_clean)
```

