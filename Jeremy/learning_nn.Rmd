```{r}
library(keras)
# install_keras() # Uncomment this line and run it once to install the newest version of keras on your computer, then comment out this line again
library(dplyr)
```

# Read in the data

- Step 1: Read in the data and separate it out into training data and test data. The test data that we have reserved will only be used at the very end to evaluate the model.

- Step 2: Use a scaling method to standardize the data. There are different standardization methods we can use, i.e. standard scaling, min-max scaling. We'll be using min-max scaling for this dataset.

- Step 3: Split the dataset into training and testing set. 

```{r}
# Step 1:
all_data <- read.csv('train.csv')

# Step 2: 
train <- all_data[,sapply(all_data, is.numeric)] %>%
  sapply(function(x) (x - min(x)) / (max(x) - min(x))) %>% # applies the function to perform min-max scaling
  as.data.frame() %>%
  select(-LotFrontage,-MasVnrArea,-GarageYrBlt) %>% # filters out all non-numeric columns as the model doesn't know how to handle strings
  na.omit()

test <- train[0:500,] # the first 500 rows belongs to the test set
train <- train[500:nrow(train),] # the rest of the dataset belongs to the training set
```

# Seperate out the output columns

For the training and testing data, we separate out the columns that will be used for output into and X and Y datasets. The X dataset will have all of the columns that are used as predictors, the Y dataset will be used to tell the model what the outcome was from the corresponding row in the X dataset.

```{r}
x_train <- train %>%
  select(-SalePrice,-Id)
y_train <- train %>%
  select(SalePrice)
x_test <- test %>%
  select(-SalePrice,-Id)
y_test <- test %>%
  select(SalePrice)
```

# Reshaping the data

We then have to reshape the data into a 2D array and get rid of the column names. The 2D array's new shape must match the shape of the input layer of the neural network, which is 33 for this dataset. Reshaping the data is a crucial step in preparing the data for neural network training. It ensures that the input data matches the shape expected by the neural network, reduces the number of dimensions, and optimizes memory usage, making the training process more efficient and effective.


```{r}
# reshape
x_train <- x_train %>%
  unlist() %>%
  array(dim = c(nrow(x_train),33))

x_test <- x_test %>%
  unlist() %>%
  array(dim = c(nrow(x_test),33))
```

```{r}
y_train <- y_train %>%
  unlist() %>%
  array(dim=c(nrow(y_train),1))
y_test <- y_test %>%
  unlist() %>%
  array(dim=c(nrow(y_test),1))
```

# Build a neural network on the data!

- Step 1: Prepare the linear stack of layers using `keras_model_sequential()`

- Step 2: Use `layer_dense()` to add a densely-connected neural network layer to the framework built previously. 

+ `units` allows us to specify the number of neurons you want to use in this model. `units = 256` creates a dense layer with 256 neurons. 

+ `activation` allows us to determine whether a neuron should be activated or not. Some common activation functions are ReLU, sigmoid, and tanh. 

+ `input_shape` is the dimensionality of the input aka the number of columns in the dataset. This argument is required when using this layer as the first layer in a model.

- Step 3: Use `layer_dropout()` to apply dropout to the input. Dropout is a technique used in neural networks to prevent overfitting. This technique drops out some neurons in the training set, which allows other neurons to be more well-rounded aka to pick up patterns to generalize on data they have never seen before. `rate` parameter ranges from 0 to 1, allowing us to specify the fraction of the neurons that we should drop along the way.

```{r}
model <- keras_model_sequential() 
model %>% 
  layer_dense(units = 256, activation = 'relu', input_shape = c(33)) %>% 
  layer_dropout(rate = 0.4) %>% 
  layer_dense(units = 128, activation = 'relu') %>%
  layer_dropout(rate = 0.3) %>%
  layer_dense(units = 64, activation = 'relu') %>%
  layer_dropout(rate = 0.2) %>%
  layer_dense(units = 1, activation = 'linear')
```

```{r}
summary(model)
```

```{r}
model %>% compile(
  loss = 'mean_squared_error',
  optimizer = 'adam',
  metrics = c('mae')
)
```

```{r}
history <- model %>% fit(
  x_train, y_train, 
  epochs = 300, batch_size = 256, 
  validation_split = 0.2
)
```

```{r}
plot(history)
```

```{r}
results <- model %>% evaluate(x_test, y_test)
results[2] * (max(all_data$SalePrice) - min(all_data$SalePrice)) + min(all_data$SalePrice)
```
