[
  {
    "objectID": "school_insights.html#school-level-academic-achievement-across-california",
    "href": "school_insights.html#school-level-academic-achievement-across-california",
    "title": "Beyond the Report Card: What are the insights at the school level that we can observe?",
    "section": "School-level academic achievement across California",
    "text": "School-level academic achievement across California\nLet’s now take a closer look at the statistics on the individual school level. To create this map, we find the GCS score for each school and divide the schools into 5 categories, then we place a marker at the approximate location based on available latitude and longitude data. Note that the range of GCS scores is much wider now since we are looking at individual schools rather than the county average.\n\n\n\n\n\n\n Schools closer to urban areas appear to have higher achievement than schools in rural areas. Zooming in on just the San Francisco Bay we see that there is a tremendous amount of variation in achievement: schools in San Jose are years behind while a swath of schools in Palo Alto, Mountain View, and Cupertino boast testing achievement more than three years ahead. On the east side of the bay in Oakland, schools just blocks apart are separated by multiple years in achievement. Further afield, Los Angeles and Sacramento have areas of very low achievement then areas of higher achievement further out, almost resembling a bullseye. How do we account for this variation?"
  },
  {
    "objectID": "school_insights.html#does-funding-impact-school-level-academic-achievement",
    "href": "school_insights.html#does-funding-impact-school-level-academic-achievement",
    "title": "Beyond the Report Card: What are the insights at the school level that we can observe?",
    "section": "Does funding impact school-level academic achievement?",
    "text": "Does funding impact school-level academic achievement?\nAdvocacy groups have long debated the influence of funding on academic achievement. Conservative groups like The Heritage Foundation note that funding, in real terms, has increased significantly since the 70s but achievement has stayed flat. They argue that more funding will do little to improve outcomes (Lips, Watkins, and Fleming 2008). On the other hand, an assortment of studies find that funding does have a measurable impact on achievement, but how the money is spent important (Burnette 2019).\nWe map California school per-pupil funding data in four spending categories. There are schools with funding well in excess of $20,000 per student, but most are accurately represented by these categories.\n\n\n\n\n\n\n There are a striking number of schools with no funding data. Nevertheless, the types of areas that are excluded don’t appear to follow a clear trend.\nAmong schools with available funding data, correlations with achievement are confusing. The high-achieving schools we saw earlier in the Palo Alto area receive very high funding but, in Oakland, the schools with the highest funding have some of the lowest levels of achievement. South of Los Angeles, the high achieveing schools in Huntington Beach are funded at levels close to and below $10,000 per student.\n\n\n\n\n\nOverall, regardless of area, higher per pupil funding is associated with lower growth scores. In California schools receive a base per-pupil grant dependent on the grades taught in the school. Schools receive supplemental funding based on the percent of students eligible for free and reduced lunch, the percent in the foster care system, and the percent that have limited English. We use a linear model of funding based solely on the percent of students eligible for free and reduced lunch to adjust funding. The negative relationship dissapears:\n\n\n\n\n\n\n\n\nIn the suburbs and city there appears to be a slight positive relationship between funding and achievement but our data and analysis really is not suited to pinpoint that effect.\nThere are still things we can take away from this analysis. Socioeconomic status, measured as the percent of students eligible for free and reduced lunch, significantly effects achievement. Additional funding received by schools with a high proportion of disadvantaged students does not affect achievement enough to offset the impact of socioeconomic status. The Education Trust finds that California’s school funding scheme is relatively progressive when comparing high and low-poverty school districts (Morgan and Amerikaner 2018). When adjusting for the additional needs of low income students, however, they say that California is merely ‘neutral.’"
  },
  {
    "objectID": "result.html",
    "href": "result.html",
    "title": "Beyond the Report Card: Let’s discuss some results!",
    "section": "",
    "text": "Add result here\n\nGrowth score is pretty robust since there are no variables that actually predict it."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Beyond the Report Card: Investigating the Factors that Define Educational Achievement using California as a Case Study",
    "section": "",
    "text": "Hi everyone, we are Thu, Nathaniel, and Jeremy, and welcome to our Capstone Project for STAT 456: Projects in Data Science. As you can tell from the title, we are interested in exploring the factors that impacting academic achievement, using California as a case study. Meet the team…\n\nYou can explore the diverse aspects of our project by navigating through various tabs on this website. Starting from our project’s inspiration, you can delve into the analysis of academic achievement insights at the county and school levels in California, learn about our model’s construction process, and discover some of the outcomes. Are you excited? Let’s get started!"
  },
  {
    "objectID": "wideformatting.html",
    "href": "wideformatting.html",
    "title": "Untitled",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nlibrary(readxl)\nlibrary(MetBrewer)\nlibrary(sf)\n\nLinking to GEOS 3.10.2, GDAL 3.4.1, PROJ 8.2.1; sf_use_s2() is TRUE\n\nlibrary(rnaturalearth)"
  },
  {
    "objectID": "wideformatting.html#data-sources",
    "href": "wideformatting.html#data-sources",
    "title": "Untitled",
    "section": "Data Sources",
    "text": "Data Sources\n\nschool_data\nCodebook\n\nschool_data <- read.csv('AllDatasets/ca_education.csv') # dataset of public K-12 spending by school\n\nschool_data_clean <- school_data %>%\n  filter(flag_nerds == 'false') %>%\n  filter(flag_f33 != '1')\n\nschool_data identified by ncesid at school level – 10404\n\n\nela_metric_data\nCodebook\n\nela_metric_data <- read.csv('AllDatasets/ca_edu_metrics.csv') # 2022 Academic Indicator (English Language Arts/Literacy) Data File\n\nela_metric_data_clean <- ela_metric_data %>%\n  filter(cds > 0, rtype == 'S') %>% # school record\n  select(-color, -box) %>%\n  mutate(cds_standardized = as.character(paste0(\"0\", cds)))\n\ncolnames(ela_metric_data_clean) <- paste0(\"ela_data_\", colnames(ela_metric_data_clean))\nvaluecols<-names(ela_metric_data_clean)[10:21]\n\nela_wide <- ela_metric_data_clean %>% pivot_wider(names_from = ela_data_studentgroup, values_from = valuecols, names_sep = \"_\") %>% mutate(ela_data_cds = as.character(ela_data_cds))\n\nWarning: Using an external vector in selections was deprecated in tidyselect 1.1.0.\nℹ Please use `all_of()` or `any_of()` instead.\n  # Was:\n  data %>% select(valuecols)\n\n  # Now:\n  data %>% select(all_of(valuecols))\n\nSee <https://tidyselect.r-lib.org/reference/faq-external-vector.html>.\n\n\nela_metric_data identified by cds at school level – 9845 AND student group – 165532\n\n\nela and math metrics\n\nela_mth_proficiency_ranges<-read.csv(\"AllDatasets/nathanieldata.csv\") %>% filter(GRADE == \"00\" & CATEGORY == \"ALL\") %>% mutate(NCESSCH = as.character(NCESSCH), NCESSCH = paste0(\"0\",NCESSCH))\n\n\n\nscience_metric_data\nCodebook\n\nscience_metric_data <- read.csv('AllDatasets/science_ca_assesment.csv', sep = \"^\")\n\nscience_metrics_clean <- science_metric_data %>%\n  mutate(cds = as.character(paste0(0, paste0(paste0(County.Code,District.Code),paste0(0,School.Code))))) %>% # add cds identifier\n  # mutate(cds_standardized = ifelse(nchar(cds) == 13, as.character(paste0(0,cds)), cds)) %>% # there are school codes missing the last 0, this fixes that to match other cds\n  filter(County.Code > 0, Type.ID == 07) %>%\n  select(-County.Code, -District.Code, -School.Code, -Filler) \n\n\nvaluecols <- c(names(science_metrics_clean)[8:25],names(science_metrics_clean)[4:5])\nscience_metrics_wide <- science_metrics_clean %>% pivot_wider(names_from = Grade, values_from = valuecols)\ncolnames(science_metrics_wide) <- paste0(\"science_data_\", colnames(science_metrics_wide))\n\ncolnames(science_metrics_clean) <- paste0(\"science_data_\", colnames(science_metrics_clean))\n\nscience_metrics_clean identified by cds_standardized at school level – 8819 AND Grade – 25298\n\n\nstanford_data and cov\nCodebook\nCovariate Codebook\n\nstanford_data <- read.csv('AllDatasets/seda_school_pool_gcs_4.1.csv')\nstanford_cov <- read.csv(\"AllDatasets/seda_cov_school_pool_4.1.csv\")\n\nstanford_cov <- stanford_cov %>% filter(stateabb == \"CA\")\n\nstanford_data_clean <- stanford_data %>% \n  filter(stateabb == 'CA') %>% \n  select(sedasch, sedaschname, fips, stateabb, subcat, subgroup, gradecenter, gap, contains(\"avg\"), -ends_with(\"se\")) %>%\n  mutate(standardized_school_id = paste0(\"0\", sedasch)) %>% # adding 0 in front of school id to match the format of the school dataset\n  left_join(stanford_cov)\n\nJoining with `by = join_by(sedasch, fips, stateabb)`\n\n\nstanford_data AND stanford_cov identify eachother by sedasch\nstanford_data AND stanford_cov identified by standardized_school_id – 8512\n\n\nca_school_details\n\nca_school_details <- read_excel(\"AllDatasets/ca_school_details.xlsx\") # CA schools metadata\n\nca_school_details_clean <- ca_school_details %>%\n  filter(StatusType == \"Active\") %>%\n  filter(School != \"No Data\") %>%\n  mutate(school_id = paste0(NCESDist, NCESSchool))\n\nca_school_details_clean identified by school_id – 10629\n\n\ngrowth_aggr\n\ngrowth_aggr <- read_excel(\"AllDatasets/growthaggr.xlsx\")\n\ngrowth_aggr_clean <- growth_aggr %>% filter(rtype == \"S\") %>% \n  filter(studentgroup == \"ALL\") %>%\n  pivot_wider(names_from = subject, values_from = c(n_growthscores, growthscore, decilerank))"
  },
  {
    "objectID": "wideformatting.html#joining",
    "href": "wideformatting.html#joining",
    "title": "Untitled",
    "section": "Joining",
    "text": "Joining\n\nwide_merged_data <- ca_school_details_clean %>% \n  left_join(school_data_clean, by=c('school_id'='ncesid')) %>%\n  left_join(science_metrics_wide, by = c(\"CDSCode\"=\"science_data_cds\")) %>%\n  left_join(ela_wide, by = c(\"CDSCode\"=\"ela_data_cds_standardized\")) %>% \n  left_join(stanford_data_clean, by=c('school_id'='standardized_school_id')) %>% \n  mutate(ncesid = paste0(NCESDist,NCESSchool)) %>%\n  left_join(growth_aggr_clean, by = c('CDSCode' = 'cds')) %>%\n  left_join(ela_mth_proficiency_ranges, by=c(\"ncesid\"=\"NCESSCH\"))\n\n\n# save(wide_merged_data, file=\"WideMergedData.RData\")\n\n# names(wide_merged_data)\n\n\n# length(unique(wide_merged_data$CDSCode)) #10629 unique schools from the metadata\n# length(unique(wide_merged_data$CDSCode[is.na(wide_merged_data$schoolname) == FALSE])) #4385 unique matched schools for Jeremy's california school data\n# length(unique(wide_merged_data$CDSCode[is.na(wide_merged_data$sedasch) == FALSE])) #7390 unique matched schools for Stanford data\n# length(unique(wide_merged_data$CDSCode[is.na(wide_merged_data$ela_data_cds) == FALSE])) #814 unique matched schools for ela data\n# length(unique(wide_merged_data$CDSCode[is.na(wide_merged_data$science_data_cds) == FALSE])) #495 unique matched schools for science data\n# length(unique(wide_merged_data$CDSCode[is.na(wide_merged_data$PE_data_cds) == FALSE])) #501 unique matched schools for pe data\n\n\n# wide_merged_data %>% filter(!is.na(urbanicity)) %>%\n#   ggplot(aes(x=as.numeric(pp_total_norm_NERDS), y=(as.numeric(gcs_mn_avg_ol)-as.numeric(gradecenter)), color=urbanicity)) +\n#   geom_point(alpha = .1) + \n#   geom_smooth(method = lm) + \n#   facet_wrap(~urbanicity) + \n#   scale_x_continuous(limits = c(8000,18000)) + \n#   theme_nr()+\n#   labs(y=\"Grade Cohort Score\", x= \"Per Student Spending\") + guides(color=FALSE) +   geom_hline(yintercept = 0)\n\n\n# wide_merged_data %>% ggplot(aes(x=as.numeric(science_data_Percentage.Standard.Met.and.Above_13), y=(as.numeric(MTH_PCTPROF_max)))) +\n#   geom_point(alpha = .3) + \n#   geom_smooth(color=\"grey\",method=lm) + theme_nr() + labs(x = \"Percent Proficient on State Science Test\", y=\"Maximum* Percent Proficient on NAEP Math Test\", caption = \"*True value is hidden to protect student privacy at smaller schools\")\n# \n# wide_merged_data %>%\n#   ggplot(aes(x=as.numeric(perhsp), y=(as.numeric(perfrl)))) +\n#   geom_point(alpha = .05) + \n#   geom_smooth(color=\"black\") + theme_nr() + labs(x = \"Percent Hispanic\", y=\"Percent Eligible fro Free and Reduced Lunch\")\n# \n# \n# wide_merged_data %>% filter(!is.na(urbanicity)) %>%\n#   ggplot(aes(x=as.numeric(perfrl),y=(as.numeric(gcs_mn_avg_ol)-as.numeric(gradecenter)),color=State)) +\n#   geom_point(alpha = .05) +\n#   geom_smooth(method=lm)+\n#   guides(color=FALSE) + \n#   theme_nr()+\n#   labs(x=\"Percent Eligible for Free and Reduced Lunch\", y=\"Grade Cohort Score\")+\n#   geom_hline(yintercept = 0)\n# \n# wide_merged_data %>% filter(!is.na(urbanicity)) %>%\n#   ggplot(aes(x=as.numeric(perhsp),y=(as.numeric(gcs_mn_avg_ol)-as.numeric(gradecenter)),color=State)) +\n#   geom_point(alpha = .05) +\n#   geom_smooth()+\n#   guides(color=FALSE) + \n#   theme_nr()+\n#   labs(x=\"Percent Hispanic\", y=\"Grade Cohort Score\")+\n#   geom_hline(yintercept = 0)\n\n\n# counties <- st_as_sf(maps::map(\"county\", plot = FALSE, fill = TRUE))\n# counties <- subset(counties, grepl(\"california\", counties$ID))\n# world <- ne_countries(scale = \"medium\", returnclass = \"sf\")\n# \n# ggplot(data = world) +\n#   geom_sf(data = counties, fill='#f0f1f9') + \n#   geom_point(data = (wide_merged_data%>%filter(!is.na(gradecenter)&!is.na(gcs_mn_avg_ol))) , aes(x=as.numeric(Longitude),y=as.numeric(Latitude),color=(as.numeric(gcs_mn_avg_ol)-as.numeric(gradecenter)),size=as.numeric(ncesenroll)), alpha =.5) +\n#   theme_classic() +\n#   scale_color_met_c(\"Benedictus\",direction = 1) +\n#   scale_size_area(max_size = 2) + theme_nr() +\n#   labs(x=\"Longitude\",y=\"Latitude\",color=\"Grade Cohort Score\",title = \"California School Location and Academic\\nPerformance\")+guides(size=FALSE)+theme(legend.position = c(.8,.8))\n# \n# # ----------------------------------------------------\n# ggplot(data = world) +\n#   geom_sf(data = counties, fill='#f0f1f9') + \n#   geom_point(data = (wide_merged_data%>%filter(!is.na(gradecenter)&!is.na(gcs_mn_avg_ol))) , aes(x=as.numeric(Longitude),y=as.numeric(Latitude),color=(as.numeric(gcs_mn_avg_ol)-as.numeric(gradecenter)),size=as.numeric(ncesenroll)), alpha =.5) +\n#   theme_classic() +\n#   scale_color_met_c(\"Benedictus\",direction = 1) +\n#   scale_size_area(max_size = 7) + theme_nr() +\n#   labs(x=\"Longitude\",y=\"Latitude\",color=\"Grade Cohort Score\",title = \"Bay Area Academic Performance\")+guides(size=FALSE)+theme(legend.position = c(.28,.3)) +\n#   coord_sf(xlim=c(-121.25,-123.5), ylim= c(36.75,38.75))\n# \n# # -------------------------------------------------\n# ggplot(data = world) +\n#   geom_sf(data = counties, fill='#f0f1f9') + \n#   geom_point(data = (wide_merged_data%>%filter(!is.na(gradecenter)&!is.na(gcs_mn_avg_ol))) , aes(x=as.numeric(Longitude),y=as.numeric(Latitude),color=(as.numeric(gcs_mn_avg_ol)-as.numeric(gradecenter)),size=as.numeric(ncesenroll)), alpha =.5) +\n#   theme_classic() +\n#   scale_color_met_c(\"Benedictus\",direction = 1) +\n#   scale_size_area(max_size = 2) + theme_nr() +\n#   labs(x=\"Longitude\",y=\"Latitude\",color=\"Grade Cohort Score\",title = \"Los Angeles and San Diego Academic\\nPerformance\")+guides(size=FALSE)+theme(legend.position = c(.3,.3)) +\n#   coord_sf(xlim=c(-117,-120), ylim= c(32,35))\n\n\n# ggplot(data = world) +\n#   geom_sf(data = counties, fill='#f0f1f9') + \n#   geom_point(data =((wide_merged_data %>% filter(!is.na(pp_total_norm_NERDS) & as.numeric(pp_total_norm_NERDS) > 7000 & as.numeric(pp_total_norm_NERDS) < 25000))), aes(x=as.numeric(Longitude),y=as.numeric(Latitude),color=as.numeric(pp_total_norm_NERDS),size=as.numeric(ncesenroll)), alpha =.25) +\n#   theme_classic() +\n#   scale_color_met_c(\"Benedictus\",direction = 1) +\n#   scale_size_area(max_size = 2) + theme_nr() +\n#   labs(x=\"Longitude\",y=\"Latitude\",color=\"Per Student Spending\",title = \"California School Location and Per Student\\nSpending\")+guides(size=FALSE)+theme(legend.position = c(.8,.8))"
  },
  {
    "objectID": "IntermediateNarrative.html#analyses-with-a-short-description-of-results",
    "href": "IntermediateNarrative.html#analyses-with-a-short-description-of-results",
    "title": "Beyond the Report Card: Investigating the Factors that Define Educational Achievement",
    "section": "Analyses with a short description of results",
    "text": "Analyses with a short description of results\nThu’s results\nThis map shows the deviation in SEDA scores from the national standard for each county, providing a broad overview of academic achievement levels in Californian counties. Overall, areas in cities or richer suburbs (Silicon Valley, Los Angeles, San Diego) have higher academic performance than national average, shown through brigher colors (yellow and green). On the other hand, areas with fewer schools (e.g. Inyo – a national forest area) have lower academic performance than national average, denoted by darker (blue and purple) colors.\n\n\n\n\n\n\n\n\n\nJeremy’s results\nWe examined solely the schools for which we have complete data available, i.e., the schools in the intersection of our datasets. Among these schools, across all metrics, an increase in per-pupil government spending (both state and federal) showed a negative correlation with performance. At first glance, the outcomes appear to be linked to the increase in per-pupil spending when there is a significant percentage of students who are English learners, in the foster care system, or eligible for free/reduced lunch.\n\n\n\nNathaniel’s results\n\n\n\nSpending per student varies significantly across the state. We thought that this might explain some of the funding-achievement issue we had come across. After seeing that the same negative relationship existed to some degree in each locale we decided that location, or at least type of area, was not the factor we were looking for. It is also worth noting that the ‘Rural’ and ‘Town’ areas seem to be under performing for some reason.\n\n\n\n\n\n\n\n\n\nWe understand that funding for schools in California is based in part on the percent of students eligible for free and reduced lunch, the percent in the foster care system, and the percent that have limited English. So we decided to fit some simple models of funding and these variables. We then adjusted per student spending based on the simplest model: funding ~ free and reduced lunch. This flattened the relationship between funding and achievement and flipped the relationship between percent Hispanic and spending.\n\n\n\nThe flattened relationship persists in each area.\nThe percent of students eligible for free and reduced lunch directly influences spending in the state of California. However, in wealthy localities with strong property tax bases, funding often exceeds the amount allotted by the state of California. These areas also have a lower percent of students eligible for free and reduced lunch. So our model is not looking directly at the supplemental grant received for the percent of students eligible for free and reduced lunch. Nevertheless we think it provides a more accurate picture than the raw spending metric.\nThe adjustment is very very rudimentary and would benefit from more investigation. We could probably use school location in conjunction with census data to get some picture of property taxes and local funding, but variation in local tax and funding structures could make this difficult."
  },
  {
    "objectID": "IntermediateNarrative.html#project-plan",
    "href": "IntermediateNarrative.html#project-plan",
    "title": "Beyond the Report Card: Investigating the Factors that Define Educational Achievement",
    "section": "Project plan",
    "text": "Project plan\nMoving forward, we would love to:\n\nExpand further on funding metrics and explore ways to adjust them in a way that they don’t give a false picture when taken out of context (Nathaniel has put in some work in this regard and we have a plan to accomplish this)\nIdentify more potential metrics through bivariate visualizations and adjust them so that we can put all of them together into a model that explains different education proxies\nTell a better story with missing data (for example, why certain data is missing, where is it from, where does the data in our different datasets overlap and where does it not, are there trends here or not)"
  },
  {
    "objectID": "IntermediateNarrative.html#summary-of-contributions",
    "href": "IntermediateNarrative.html#summary-of-contributions",
    "title": "Beyond the Report Card: Investigating the Factors that Define Educational Achievement",
    "section": "Summary of contributions",
    "text": "Summary of contributions\nThu, Nathaniel, and Jeremy all contribute equally to this checkpoint. Specifically:\n\nThu was responsible for cleaning and standardizing school identifiers to merge all datasets together in long format. She also cleaned the data for and visualized the general map displaying the deviation in SEDA scores from the national standard for each county, which provides a broad overview of academic achievement levels in Californian counties. Lastly, she consolidated the narration for the results and also this write-up using all of Jeremy and Nathaniel’s inputs, and organized the slides for the intermediate presentation.\nNathaniel was responsible for cleaning and merging the dataset into a wide format, as well as creating several visualizations that aided us in developing a narrative about the funding for this intermediate visualization. Additionally, he proposed a new concept for modeling an adjusted funding metric, which will provide us with a more comprehensive understanding of the correlation between funding and academic achievement at the school level.\nJeremy was responsible for gathering and aggregating some of the initial datasets that Thu then later merged with other datasets. He also conducted various analyses to compare the trend of different educational proxies as school funding increases. Due to his extensive knowledge about California, he is the primary result interpreter of the team. This enables us to contextualize the outcomes and generate ideas for future steps. Furthermore, he played a role in exploring different potential variables by cleaning the data for and visualizing various bivariate graphs."
  },
  {
    "objectID": "county_insights.html",
    "href": "county_insights.html",
    "title": "Beyond the Report Card: What are the insights at the county level that we can observe?",
    "section": "",
    "text": "An overall look at Academic Achievement across Californian counties\nIn this analysis, we define academic achievement as the difference between the Grade Cohort Standardize (GCS) Scores and NAEP standard. As a reminder of the interpretation for GCS scores, we can look at this example again: If 4th-grade students at the school of interest have a GCS value of 5.03, these students’ scores indicate a level equivalent to 5th-grade, which is about one grade level higher than the national average (the reference group) in math.\nTo create this map, we calculated the difference in GCS scores from the grade levels for each grade-school cohort, then aggregated them by the Californian counties. We divided the score differences into 3 categories: 1-2 years behind, Less than 1 year behind, and Less than 1 year ahead.\n\n\n\n\n\n\n\n\n\n Looking at this map, it is clear that there is a significant difference in academic performance across various regions in California. The Bay Area and Orange County appear to outperform other counties, along with affluent tourist counties like Placer and El Dorado. However, for the remaining counties, while approximately half of them exhibit students with academic performance on par with the national average, a significant proportion of counties have students lagging behind by 1-2 years.\nWe recognize that there may be a huge wealth disparity among diverse communities in California. Therefore, our next step is to delve deeper into the social factors that may be contributing to what we’re observing on this map.\n\n\nFurther exploration into the county-level math and reasoning language arts (RLA) achievement of different student groups\nIndeed, according to Cano and Hong, California faces challenging income disparities and intricate demographics that extend beyond its public education system. Since 2008, California has implemented extensive reforms aimed at allocating more resources to high-needs students and addressing educational disparities, with some success. However, despite these efforts, Black, Latino, and low-income students continue to significantly underperform compared to Asian American, white, and wealthier students in both math and reading proficiency (Cano and Hong 2022). A UCLA research shows that Californian schools are the most segregated for Latinos, with 58% attending schools that are highly segregated. Additionally, The study also revealed that over 50% of Black students in California are concentrated in only 25 out of 1,000 school districts (Frankenberg et al. 2019).\nSince we’re interested in how different socioeconomic backgrounds might influence education achievement, we looked into how students classified as economically disadvantaged according to the Californian standard perform compared to those in the non-economically disadvantaged group in terms of GCS scores. By definition, students are typically considered economically disadvantaged if they come from a low-income household or meet other criteria for poverty, such as eligibility for free or reduced-price meals, homelessness, or foster care. The criteria for California can be found here. The GCS score differences are broken down into 6 categories, ranging from being 2+ years behind to 2+ years ahead compared to the NAEP standard. The GCS categories are also divided into two disciplines, i.e. math and RLA for both economically disadvantaged and non-economically disadvantaged groups.\n\n\n\n\n\n\n\n\n\n\n\n\n Although it is intuitive that economically disadvantaged students perform worse than their non-economically disadvantaged peers, it is still not at all less shocking when we looked at this map for the first time. The general trend shows that at county level, there is almost no overlapping in academic achievement between economically disadvantaged and non-economically disadvantaged student groups, for both math and RLA. Specifically, while the economically disadvantaged group is lagging behind by 1 to 2+ years behind the national standard, the more privileged group performs much better, having a performance gap of less than 1 year to being 2+ years ahead of the national standard.\n\n\n\n\n\nReferences\n\nCano, Ricardo, and Joe Hong. 2022. “Mind the Achievement Gap: California’s Disparities in Education, Explained.” Calmatters. https://calmatters.org/explainers/achievement-gap-california-explainer-schools-education-disparities-explained/.\n\n\nFrankenberg, Erica, Jongyeon Ee, Jennifer B. Ayscue, and Gary Orfield. 2019. “Harming Our Common Future: America’s Segregated Schools 65 Years aFter Brown.” The Civil Rights Project. www.civilrightsproject.ucla.edu."
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "Beyond the Report Card: Our Data",
    "section": "",
    "text": "A sneak peek into the data\nThe data set we have includes a range of school-related variables such as location details, funding, and aggregated scores in various subjects. Specifically, the score variables cover the general grade-cohort-standardized achievement score, as well as scores in reading, science, and physical education.\nWe aggregate our dataset from 5 different datasets. We use data from The California Department of Education, Georgetown University, and the Educational Opportunity Project at Stanford University.\nScience Testing Data Codebook\nOur science test data is from the California Department of Education, specifically, from the 2021-2022 school year. It is from the California Science Test, in which there are three different categories, namely Life sciences, Physical sciences, and Earth and Space sciences.\nEnglish Testing Data Codebook\nThe data we have for English Language Arts / Literature is also from the California Department of Education, specifically from 2022. It tells us for each student group within each school their level of proficiency.\nPhysical Education Data Codebook\nOur PE data comes from the California Department Education from the 2018-2019 school year. It has 7 different types of exercises and each school’s grade’s proficiency on each type of exercise.\nSchool Funding Data Codebook\nOur school funding data is aggregated 2019-2020 data from different federal and state sources. It is compiled into the dataset we are using by Georgetown University researchers. It has information about funding going to a school from the state, local, and federal governments, as well as metadata about the school such as enrollment, as well as data about the income levels of the students at the school.\nThe Educational Opportunity Project at Stanford University (SEDA) Codebook Covariate Codebook\nThe SEDA dataset contains school-level standardized academic achievement data across all Californian schools. These achievement scores are graded and cohort standardized against the NAEP standard, indicating whether the students in a particular school and grade level are meeting the national standard for their grade. For instance, if a school’s 4th-grade students score 3.5, it indicates that they are lagging behind the national standard by 0.5 points. The achievement estimates are calculated using Ordinary Least Square (OLS) and Empirical Bayesian (EB) techniques.\nSchool Details Codebook\nThis contains metadata on 10629 California Schools, including both the nationally used school identifier, or NCES ID, and the California CDS code. We use this to join our data from our different sources. This dataset also contains the longitude and latitude of the schools which has been very useful for EDA so far.\nAggregated Growth Codebook\nThis contains data on Mathematics and English Language Arts growth by school and student group for California Schools. California developed the growth model to show how much student’s scores were growing from year to year.\n\n\nGoing into details regarding the metrics considered in this analysis\nIn this analysis, we focused on using Grade Cohort Standardized scores, which is a metric created by the SEDA project, and growth score (?), both at county- and school-level. Additionally, we also considered school-level test scores in various subjects, including English, Science, and Physical Education (PE).\nGrade Cohort Standardized (GCS) scores: According to the Stanford Codebook, “[t]he GCS scale standardizes the unit means relative to the average difference in NAEP scores between students one grade level apart” (“Methods: How SEDA Produced Estimates of Student Performance That Are Comparable Across Places, Grades, and Years.” 2022). NAEP stands for National Assessment of Educational Progress, which is an assessment program in the United States that aiming to assess and measure the academic performance of students across different subjects and grades, generally referred to as the Nation’s Report Card (“About NAEP” 2023). The GCS scale allows users to understand one unit as being equivalent to one grade level. In Grade 3, the national average performance is 3 units, while in Grade 4, it is 4 units, and so on. Here are two brief examples to interpreting GCS values: If 4th-grade students at the school of interest have a GCS value of 5.03, these students’ scores indicate a level equivalent to 5th-grade, which is about one grade level higher than the national average (the reference group) in math. Vice versa, if 3rd-grade students at the school of interest have a GCS value of 2.39, these students’ reading scores indicate a level about half a grade level lower than the national average for 3rd-grade students (“Methods: How SEDA Produced Estimates of Student Performance That Are Comparable Across Places, Grades, and Years.” 2022). GCS values are aggregated over the years of 2009, 2011, 2013, and 2015, and and are collected at school- and county-level.\nGrowth score: Aggregate growth scores are available for English and Mathematics broken down by school and student group. They measure how close the students growth is to their expected growth. A growth score of 100 indicates students are meeting expected growth; below 100 indicates growth below expectations; and above 100 indicates growth above expectations. Since the scores measure growth they are based on multiple years of testing data: the 2016-17, 2017-18, and 2018-19 school years. Pandemic disruption means that the current growth scores are not actionable. Growth scores will be released again in 2024. Nevertheless, the scores are still useful for our analysis.\nSubject-specific test scores: We obtained data on test scores for various subjects, including English, Science, and PE. During our initial investigation, after joining these test score datasets with the SEDA and funding data, we saw that the test scores only matched up with a very small percentage of schools in our dataset. Further investigation shows that the test scores are only collected in the East Bay area and various northern CA counties. Since it is desirable to have a full coverage of state scores for the subsequent analysis and modeling process, we would only use these datasets in a small part of the analysis and will be mindful when discussing our results.\n\n\nComparing Performance Metrics\n\n\n\n\n\n\n\n\nOverall, GCS and growth scores seem to be pretty robust in the presence of funding compared to other metrics. Such robustness reinforces our choice of these two metrics as the primary indicators of academic accomplishment. You might notice that there’s a counter-intuitive trend between ELA, Science, and PE scores and funding, which we’ll explore and handle further in the subsequent analyses.\n\n\n\n\n\nReferences\n\n“About NAEP.” 2023. National Assessment Governing Board. https://www.nagb.gov/naep/about-naep.html.\n\n\n“Methods: How SEDA Produced Estimates of Student Performance That Are Comparable Across Places, Grades, and Years.” 2022. The Educational Opportunity Project at Stanford University. https://edopportunity.org/methods/."
  },
  {
    "objectID": "motivation.html",
    "href": "motivation.html",
    "title": "Beyond the Report Card: Our Motivation",
    "section": "",
    "text": "A change in direction\nInitially, our focus was on the correlation between affordable housing and educational achievement. However, during the process of selecting appropriate educational proxies, we found ourselves delving into the factors behind the measurements of these proxies. As a result, we decided to seek the guidance of Professor Lesley Lavery, who specializes in public policies in education. We hope to gain a better understanding of the educational proxies currently in use and to investigate whether there are additional factors that impact educational outcomes and potentially render the current proxies interchangeable or distinct from other measures.\n\n\nPrevious literature\nThere are several reasons why education proxies differ, including the influence of socioeconomic status, the difficulty in standardizing scores across schools, and variations in school budgets. While % of high school graduates may be a more robust metric to estimate the education achievement level in different areas, it is harder to standardize score-related metrics due to differences in grading scales and the difficulty of obtaining student-level data (“Expert Interview with Lesley Lavery at Macalester College on 8 March 2023” 2023).\nAdditionally, due to the diverse student body in the US, it is difficult to identify specific teaching approaches that are effective for particular groups of students. The Measures of Effective Teaching Project by the Gates Foundation aimed to establish innovative teacher-evaluation systems incorporating classroom observation rubrics and student achievement growth measures. Although the Gates Foundation allocated $600 million towards this initiative, it turned out to be a challenging task, particularly in identifying effective teaching elements that positively influence students’ academic performance. Consequently, this research heightened our curiosity in examining other factors that contribute to students’ achievements beyond teaching approaches (Will 2018).\nAnother discourse associated with academic achievement is how funding is allocated for education and whether spending more helps improve academic achievement. Funding disparities remain significant on a national level. Districts with the highest poverty rates in the country receive approximately $1,000 less per student in comparison to those with the lowest poverty rates. Furthermore, the variation is almost double, around $1,800 less per student, for districts serving the highest number of students of color versus those serving the lowest. However, there’s variation in state trend: While certain states allocate considerably more funding to their most impoverished districts, others provide notably less. Furthermore, when the researchers took into account that students living in poverty require extra support to excel academically, the funding differences between high and low poverty districts appear even more severe. This indicates that merely providing equal funding is insufficient (Morgan and Amerikaner 2018). This is a great insight that motivates us to look further into funding and the associated factors that can potentially confound funding in our analysis.\n\n\nWhy California?\nVarious socioeconomic factors can affect students’ academic performance, beyond just teaching methods. Given that California is a diverse state with people from different socioeconomic backgrounds, it presents an intriguing case study for our research topic, which focuses on exploring the socioeconomic factors that influence students’ grades.\n\n\n\n\n\nReferences\n\n“Expert Interview with Lesley Lavery at Macalester College on 8 March 2023.” 2023.\n\n\nMorgan, Ivy, and Ary Amerikaner. 2018. “Funding Gaps: An Analysis of School Funding Equity Across the u.s. And Within Each State.” The Education Trust. https://files.eric.ed.gov/fulltext/ED587198.pdf.\n\n\nWill, Madeline. 2018. “‘An Expensive Experiment’: Gates Teacher-Effectiveness Program Shows No Gains for Students.” EducationWeek. https://www.edweek.org/teaching-learning/an-expensive-experiment-gates-teacher-effectiveness-program-shows-no-gains-for-students/2018/06."
  },
  {
    "objectID": "model.html#why-use-lasso",
    "href": "model.html#why-use-lasso",
    "title": "Beyond the Report Card: What are the significant predictors?",
    "section": "Why use LASSO",
    "text": "Why use LASSO\nIn order to interrogate the question of predictor significance we use a method called LASSO. LASSO (least absolute shrinkage and selection operator) is a way of narrowing down which predictors out of many options accounts for the most variation in the outcome variable. For our purposes, the outcome is the particular educational outcome metric that we are looking at and the predictors are aspects about the school."
  },
  {
    "objectID": "model.html#how-to-inrepret-a-lasso-graph",
    "href": "model.html#how-to-inrepret-a-lasso-graph",
    "title": "Beyond the Report Card: What are the significant predictors?",
    "section": "How to inrepret a LASSO graph",
    "text": "How to inrepret a LASSO graph\nLASSO gives us a way of seeing visually which predictors account for the most variation in the metric. When looking at the LASSO graphs below there are a few things to keep in mind. Every line on the graph represents an individual predictor such as “percent of the student body that is white” or “grade span of the school” etc. The predictors that are labeled are some of the most significant ones.\nA predictor is significant if its Y value stays either above or below zero for high penalty values. In practice this means that if a line for a predictor on the graph reaches zero at higher X values, it is more significant (ie. accounts for more variability in the outcome) compared to a predictor that reaches zero at low X values."
  },
  {
    "objectID": "model.html#note-on-predictors",
    "href": "model.html#note-on-predictors",
    "title": "Beyond the Report Card: What are the significant predictors?",
    "section": "Note on predictors",
    "text": "Note on predictors\nSome predictors are numerical and some are categorical. Numerical predictors are fairly straightforward. If the line for a numerical predictor is above zero then the higher that predictor is for any given school, the higher the outcome metric will be. And vise versa if the predictor is below. Numerical predictors are predictors such as demographics. There are also categorical predictors. If a line for a categorical predictor (such as whether the school serves kids from K-12 or not) is above zero then if that category is true for a school the associated metric will be higher and vise versa if it is below."
  },
  {
    "objectID": "model.html#ela-gcs-scores",
    "href": "model.html#ela-gcs-scores",
    "title": "Beyond the Report Card: What are the significant predictors?",
    "section": "ELA & GCS Scores",
    "text": "ELA & GCS Scores\nFirst we look at the significant predictors for average ELA Test score and GSC Score of each school. From these graphs we can see that the percentage of the student body that is Asian and the percent that is Black are both fairly significant predictors but the most significant is the percent on free/reduced lunch and the percent that are considered economically disadvantaged."
  },
  {
    "objectID": "model.html#ela-math-growth-scores",
    "href": "model.html#ela-math-growth-scores",
    "title": "Beyond the Report Card: What are the significant predictors?",
    "section": "ELA & Math Growth Scores",
    "text": "ELA & Math Growth Scores\nMoving on to the growth scores for ELA and Math, we observe that all the predictors converge to zero much faster than in the previous two graphs. This is evident from the absolute numbers on the X-axis, which are much smaller (around 0.01 - 0.03 penalty) compared to the previous graphs (around 0.1 - 0.15 penalty). This indicates that none of the predictors are as significant as those in the previous graphs. Nevertheless, there are some predictors that are marginally significant, including the ones identified earlier, as well as two additional categories related to the grades served by the school."
  },
  {
    "objectID": "school_insights.html",
    "href": "school_insights.html",
    "title": "Beyond the Report Card: What are the insights at the school level that we can observe?",
    "section": "",
    "text": "School-level academic achievement across California\nLet’s now take a closer look at the statistics on the individual school level. To create this map, we find the GCS score for each school and divide the schools into 5 categories, then we place a marker at the approximate location based on available latitude and longitude data. Note that the range of GCS scores is much wider now since we are looking at individual schools rather than the county average.\n\n\n\n\n\n\n Schools closer to urban areas appear to have higher achievement than schools in rural areas. Zooming in on just the San Francisco Bay we see that there is a tremendous amount of variation in achievement: schools in San Jose are years behind while a swath of schools in Palo Alto, Mountain View, and Cupertino boast testing achievement more than three years ahead. On the east side of the bay in Oakland, schools just blocks apart are separated by multiple years in achievement. Further afield, Los Angeles and Sacramento have areas of very low achievement then areas of higher achievement further out, almost resembling a bullseye. How do we account for this variation?\n\n\nDoes funding impact school-level academic achievement?\nAdvocacy groups have long debated the influence of funding on academic achievement. Conservative groups like The Heritage Foundation note that funding, in real terms, has increased significantly since the 70s but achievement has stayed flat. They argue that more funding will do little to improve outcomes (Lips, Watkins, and Fleming 2008). On the other hand, an assortment of studies find that funding does have a measurable impact on achievement, but how the money is spent important (Burnette 2019).\nWe map California school per-pupil funding data in four spending categories. There are schools with funding well in excess of $20,000 per student, but most are accurately represented by these categories.\n\n\n\n\n\n\n There are a striking number of schools with no funding data. Nevertheless, the types of areas that are excluded don’t appear to follow a clear trend.\nAmong schools with available funding data, correlations with achievement are confusing. The high-achieving schools we saw earlier in the Palo Alto area receive very high funding but, in Oakland, the schools with the highest funding have some of the lowest levels of achievement. South of Los Angeles, the high achieving schools in Huntington Beach are funded at levels close to and below $10,000 per student.\n\n\n\n\n\nOverall, regardless of area, higher per pupil funding is associated with lower growth scores. In California schools receive a base per-pupil grant dependent on the grades taught in the school. Schools receive supplemental funding based on the percent of students eligible for free and reduced lunch, the percent in the foster care system, and the percent that have limited English. We use a linear model of funding based solely on the percent of students eligible for free and reduced lunch to adjust funding. The negative relationship dissapears:\n\n\n\n\n\n\n\n\nIn the suburbs and city there appears to be a slight positive relationship between funding and achievement but our data and analysis really is not suited to pinpoint that effect.\nThere are various takeaways from this analysis. Socioeconomic status, measured as the percent of students eligible for free and reduced lunch, significantly effects achievement. Additional funding received by schools with a high proportion of disadvantaged students does not affect achievement enough to offset the impact of socioeconomic status. The Education Trust finds that California’s school funding scheme is relatively progressive when comparing high and low-poverty school districts (Morgan and Amerikaner 2018). When adjusting for the additional needs of low income students, however, they say that California is merely ‘neutral.’\n\n\n\n\n\n\n\n\nReferences\n\nBurnette, Daarel. 2019. “Student Outcomes: Does More Money Really Matter?” Education Week. https://www.edweek.org/policy-politics/student-outcomes-does-more-money-really-matter/2019/06.\n\n\nLips, Dan, Shanea Watkins, and John Fleming. 2008. “Does Spending More on Education Improve Academic Achievement?” Backgrounder. https://files.eric.ed.gov/fulltext/ED509499.pdf.\n\n\nMorgan, Ivy, and Ary Amerikaner. 2018. “Funding Gaps: An Analysis of School Funding Equity Across the u.s. And Within Each State.” The Education Trust. https://files.eric.ed.gov/fulltext/ED587198.pdf."
  },
  {
    "objectID": "model.html#why-lasso",
    "href": "model.html#why-lasso",
    "title": "Beyond the Report Card: What are the significant predictors?",
    "section": "Why LASSO",
    "text": "Why LASSO\nTo investigate the significance of predictors, we will utilize a technique known as LASSO (Least Absolute Shrinkage and Selection Operator). This approach involves identifying the most influential predictors out of numerous possibilities that explain the majority of the variation in the outcome variable. In our case, the outcome variable is the specific educational outcome metric we are analyzing, while the predictors are various characteristics of the school."
  },
  {
    "objectID": "model.html#how-to-interpret-a-lasso-graph",
    "href": "model.html#how-to-interpret-a-lasso-graph",
    "title": "Beyond the Report Card: What are the significant predictors?",
    "section": "How to interpret a LASSO graph",
    "text": "How to interpret a LASSO graph\nLASSO provides us with a visual representation of which predictors are responsible for the most variation in the metric. It’s important to note a few things while analyzing the LASSO graphs below. Each line on the graph corresponds to an individual predictor, such as “percentage of students who are white” or “school grade span,” among others. The most significant predictors are indicated on the graph.\nA predictor is considered significant if its Y-value remains consistently above or below zero for high penalty values. In other words, if the line for a predictor on the graph reaches zero at higher X-values, it is more significant, meaning it accounts for a greater amount of variability in the outcome, compared to a predictor that reaches zero at low X-values."
  },
  {
    "objectID": "model.html#gcs-scores-in-math-and-reasoning-language-arts-rla",
    "href": "model.html#gcs-scores-in-math-and-reasoning-language-arts-rla",
    "title": "Beyond the Report Card: What are the significant predictors?",
    "section": "GCS Scores in Math and Reasoning Language Arts (RLA)",
    "text": "GCS Scores in Math and Reasoning Language Arts (RLA)\nFirst, we examine the significant predictors for the average ELA Test score and GCS Score for each school. Based on these graphs, we observe that the percentage of students who are Asian and Black are relatively strong predictors. However, the most significant predictors are the percentage of students receiving free/reduced lunch and the percentage of students considered economically disadvantaged."
  },
  {
    "objectID": "data.html#a-comparison-of-different-performance-metrics",
    "href": "data.html#a-comparison-of-different-performance-metrics",
    "title": "Beyond the Report Card: Our Data",
    "section": "A comparison of different performance metrics",
    "text": "A comparison of different performance metrics\n\n\n\n\n\n\n\n\nOverall, GCS and growth scores seem to be pretty robust in the presence of funding compared to other metrics. Such robustness reinforces our choice of these two metrics as the primary indicators of academic accomplishment. You might notice that there’s a counter-intuitive trend between ELA, Science, and PE scores and funding, which we’ll explore and handle further in the subsequent analyses."
  },
  {
    "objectID": "model.html#ela-and-gcs-scores",
    "href": "model.html#ela-and-gcs-scores",
    "title": "Beyond the Report Card: What are the significant predictors?",
    "section": "ELA and GCS Scores",
    "text": "ELA and GCS Scores\nFirst, we examine the significant predictors for the average ELA Test score and GCS Score for each school. Based on these graphs, we observe that the percentage of students who are Asian and Black are relatively strong predictors. However, the most significant predictors are the percentage of students receiving free/reduced lunch and the percentage of students considered economically disadvantaged."
  }
]